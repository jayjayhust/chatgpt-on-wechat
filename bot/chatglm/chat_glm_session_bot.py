# encoding:utf-8
# REF URLï¼ˆæ–‡æœ¬chatï¼‰: 
# REF URLï¼ˆå›¾ç‰‡ç”Ÿæˆï¼‰: https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Klkqubb9w

import zhipuai
import time
import requests
import json

from bot.bot import Bot
from bot.chatglm.chat_glm_session import ChatGLMSession
from bot.session_manager import SessionManager
from bridge.context import ContextType
from bridge.reply import Reply, ReplyType
from config import conf, load_config

import openai
import openai.error
import pinecone  # pip install pinecone-client python-docx plotly scikit-learn
from openai.embeddings_utils import get_embedding  # pip install matplotlib pandas
import os
from common.log import logger

from bot.baidu.baidu_ernie_image import BaiduErnieImage

### init pinecone configuration
pinecone_api_key = conf().get("pinecone_api_key") or os.environ.get('PINECONE_API_KEY')
pinecone.init(
    # api_key="pinecone api key",
    api_key=pinecone_api_key,
    environment="eu-west1-gcp"
)

# create or connect to index
index_name = "holon-expert-2023-0509"
if index_name not in pinecone.list_indexes():
    pinecone.create_index(index_name, dimension=1536)
    logger.debug("pinecone index created!")

# connect to index(this operation shall take a while)
index = pinecone.Index(index_name)
logger.debug("pinecone index connected!")

### Query Index
def search_docs(query):
    # è¿™éƒ¨åˆ†é€»è¾‘å°†æ¥ä¹Ÿè¦æ›¿æ¢æˆå›½å†…å¤§æ¨¡å‹çš„Embeddingæ¥å£
    xq = openai.Embedding.create(input=query, engine="text-embedding-ada-002")['data'][0]['embedding']
    # pineconeçš„queryæ–¹æ³•ï¼šdef query(
    # vector: List[float] | None = None,
    # id: str | None = None,
    # queries: List[QueryVector] | List[Tuple] | None = None,
    # top_k: int | None = None,
    # namespace: str | None = None,
    # filter: Dict[str, str | float | int | bool | List | dict] | None = None,
    # include_values: bool | None = None,
    # include_metadata: bool | None = None,
    # sparse_vector: SparseValues | Dict[str, List[float] | List[int]] | None = None,
    # **kwargs: Any
    res = index.query([xq], top_k=5, include_metadata=True)
    chosen_text = []
    # for match in res['matches']:  # éå†æŸ¥è¯¢çš„ç»“æœ
    #     chosen_text = match['metadata']
    return res['matches']  # è¿”å›æŸ¥è¯¢çš„ç»“æœ


### Construct Prompt
def construct_prompt(query):
    is_in_index = False
    # is_in_index = True
    matches = search_docs(query)

    chosen_text = []
    for match in matches:  # éå†æŸ¥è¯¢çš„ç»“æœ
        chosen_text.append(match['metadata']['text'])  # æå–å•æ¡æ•°æ®çš„å…ƒæ•°æ®éƒ¨åˆ†çš„textå­—æ®µå†…å®¹
        if(match['score'] > 0.85):
            is_in_index = True

    if (is_in_index):
        # prompt = """Answer the question as truthfully as possible using the context below, and if the answer is no within the context, say 'I don't know or æŠ±æ­‰æˆ‘çš„çŸ¥è¯†åº“è¿˜æ²¡æœ‰è¿™å—çš„çŸ¥è¯†.'.Remember to reply in the same language as the Question."""
        prompt = """è¯·å°½é‡å¦‚å®å›ç­”ç”¨æˆ·çš„æé—®ï¼Œå¦‚æœç­”æ¡ˆä¸åœ¨ä¸‹è¿°æä¾›çš„èƒŒæ™¯å†…å®¹ä¸­ï¼Œè¯·ç›´æ¥å›ç­”'æŠ±æ­‰æˆ‘çš„çŸ¥è¯†åº“è¿˜æ²¡æœ‰è¿™å—çš„çŸ¥è¯†'ã€‚è®°å¾—ç”¨ç”¨æˆ·æé—®çš„è¯­è¨€æ¥é—®ç­”é—®é¢˜ã€‚"""
        prompt += "\n\n"
        # prompt += "Context: " + "\n".join(chosen_text)  # TypeError: sequence item 0: expected str instance, list found
        prompt += "æä¾›çš„èƒŒæ™¯å†…å®¹ï¼š" + "\n".join('%s' %a for a in chosen_text)
        prompt += "\n\n"
        prompt += "é—®é¢˜ï¼š" + query
        prompt += "\n"
        prompt += "å›ç­”ï¼š"
        return prompt
    else:
        return query

def get_token():
    access_key = conf().get("baidu_ernie_access_key")
    secret_key = conf().get("baidu_ernie_secret_key")
    url  = "https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=" + access_key + "&client_secret=" + secret_key
    payload = json.dumps("")
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
        
    response = requests.request("POST", url, headers=headers, data=payload)
    return response.json().get("access_token")

# Zhipu ChatGLMå¯¹è¯æ¥å£ 
class ChatGLMSessionBot(Bot):
    def __init__(self):
        super().__init__()

        self.sessions = SessionManager(ChatGLMSession, model=conf().get("model") or "chatglm_pro")
        self.args = {
            "model": conf().get("model") or "chatglm_pro",  # å¯¹è¯æ¨¡å‹çš„åç§°
            "temperature": conf().get("temperature", 0.9),  # å€¼åœ¨[0,1]ä¹‹é—´ï¼Œè¶Šå¤§è¡¨ç¤ºå›å¤è¶Šå…·æœ‰ä¸ç¡®å®šæ€§
            # "max_tokens":4096,  # å›å¤æœ€å¤§çš„å­—ç¬¦æ•°
            # "top_p": 1,
            # "frequency_penalty": conf().get("frequency_penalty", 0.0),  # [-2,2]ä¹‹é—´ï¼Œè¯¥å€¼è¶Šå¤§åˆ™æ›´å€¾å‘äºäº§ç”Ÿä¸åŒçš„å†…å®¹
            # "presence_penalty": conf().get("presence_penalty", 0.0),  # [-2,2]ä¹‹é—´ï¼Œè¯¥å€¼è¶Šå¤§åˆ™æ›´å€¾å‘äºäº§ç”Ÿä¸åŒçš„å†…å®¹
            # "request_timeout": conf().get("request_timeout", None),  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œopenaiæ¥å£é»˜è®¤è®¾ç½®ä¸º600ï¼Œå¯¹äºéš¾é—®é¢˜ä¸€èˆ¬éœ€è¦è¾ƒé•¿æ—¶é—´
            # "timeout": conf().get("request_timeout", None),  # é‡è¯•è¶…æ—¶æ—¶é—´ï¼Œåœ¨è¿™ä¸ªæ—¶é—´å†…ï¼Œå°†ä¼šè‡ªåŠ¨é‡è¯•
        }
        self.use_vector_db = conf().get("use_vector_db") or False

    def reply(self, query, context=None):
        # acquire reply content
        if context.type == ContextType.TEXT:  # æ–‡æœ¬é—®ç­”
            logger.info("[ChatGLM] query={}".format(query))
            session_id = context["session_id"]
            reply = None  # åˆå§‹åŒ–reply
            clear_memory_commands = conf().get("clear_memory_commands", ["#æ¸…é™¤è®°å¿†"])
            if query in clear_memory_commands:
                self.sessions.clear_session(session_id)
                reply = Reply(ReplyType.INFO, "è®°å¿†å·²æ¸…é™¤")
            elif query == "#æ¸…é™¤æ‰€æœ‰":
                self.sessions.clear_all_session()
                reply = Reply(ReplyType.INFO, "æ‰€æœ‰äººè®°å¿†å·²æ¸…é™¤")
            elif query == "#æ›´æ–°é…ç½®":
                load_config()
                reply = Reply(ReplyType.INFO, "é…ç½®å·²æ›´æ–°")
            if reply:  # å¦‚æœæ˜¯æŒ‡ä»¤ï¼Œç›´æ¥å›å¤
                return reply
            if self.use_vector_db:
                # åœ¨è¿™é‡Œè¿›è¡Œç§æœ‰æ•°æ®åº“çš„åˆ¤æ–­ï¼šé€šè¿‡åˆ¤æ–­ç¾¤åæ˜¯å¦åœ¨group_chat_using_private_dbä¸­çš„é…ç½®ï¼Œæ¥è®¾å®šnamespaceæ˜¯å¦éœ€è¦è®¾ç½®
                # (logic reserved here=======================================)
                group_chat_name = context["msg"].other_user_nickname
                group_chat_id = context["msg"].other_user_id
                
                # åœ¨è¿™é‡Œé‡ç»„query(åŠ è½½å‘é‡æ•°æ®åº“pineconeä¸“å®¶åº“ï¼Œå…ˆè¿›è¡Œä¸“å®¶åº“æ£€ç´¢)
                prompt = construct_prompt(query)
            else:
                # ä¸åŠ è½½å‘é‡æ•°æ®åº“
                prompt = query
            logger.debug(prompt)
            
            session = self.sessions.session_query(prompt, session_id)
            logger.debug("[ChatGLM] session query={}".format(session.messages))

            reply_content = self.reply_text(session)  # è°ƒç”¨reply_text()å¹¶ä¼ å…¥sessionå‚æ•°ï¼ˆå®ç°çŸ­æœŸè®°å¿†ï¼‰
            logger.debug(
                "[ChatGLM] new_query={}, session_id={}, reply_cont={}, completion_tokens={}".format(
                    session.messages,
                    session_id,
                    reply_content["content"],
                    reply_content["completion_tokens"],
                )
            )
            if reply_content["completion_tokens"] == 0 and len(reply_content["content"]) > 0:
                reply = Reply(ReplyType.ERROR, reply_content["content"])
            elif reply_content["completion_tokens"] > 0:
                self.sessions.session_reply(reply_content["content"], session_id, reply_content["total_tokens"])
                reply = Reply(ReplyType.TEXT, reply_content["content"], reply_content["completion_tokens"])
            else:
                reply = Reply(ReplyType.ERROR, reply_content["content"])
                logger.debug("[ChatGLM] reply {} used 0 tokens.".format(reply_content))
            return reply
        # elif context.type == ContextType.IMAGE_CREATE:  # å›¾ç‰‡ç”Ÿæˆ
        #     ok, retstring = self.create_img(query, 0)
        #     reply = None
        #     if ok:
        #         reply = Reply(ReplyType.IMAGE, retstring)
        #     else:
        #         reply = Reply(ReplyType.ERROR, retstring)
        #     return reply
        else:
            reply = Reply(ReplyType.ERROR, "Botä¸æ”¯æŒå¤„ç†{}ç±»å‹çš„æ¶ˆæ¯".format(context.type))
            return reply
        
    def reply_text(self, session: ChatGLMSession, retry_count=0) -> dict:
        """
        call ChatGLM's Chat to get the answer
        :param session: a conversation session
        :param session_id: session id
        :param retry_count: retry count
        :return: {}
        """
        try:
            # if conf().get("rate_limit_chatgpt") and not self.tb4chatgpt.get_token():
            #     raise openai.error.RateLimitError("RateLimitError: rate limit exceeded")
            # if api_key == None, the default openai.api_key will be used
            # response = openai.ChatCompletion.create(api_key=api_key, messages=session.messages, **self.args)
            # logger.info("[ERNIE] reply={}, total_tokens={}".format(response.choices[0]['message']['content'], response["usage"]["total_tokens"]))
            
            # ChatGLM API: https://open.bigmodel.cn/dev/api#chatglm_pro
            zhipuai.api_key = conf().get("zhipu_api_key")
            response = zhipuai.model_api.invoke(
                # model="chatglm_lite",  # ChatGLM-6B(https://open.bigmodel.cn/doc/api#chatglm_lite)
                # model="chatglm_std",  # ChatGLM(https://open.bigmodel.cn/doc/api#chatglm_std)
                # model="chatglm_pro",  # ChatGLM(https://open.bigmodel.cn/doc/api#chatglm_pro)
                model=conf().get("model") or "chatglm_turbo",
                prompt=session.messages,
                top_p=0.7,
                temperature=0.9,
            )
            # responseå½¢å¦‚ï¼š
            # {'code': 200, 'msg': 'æ“ä½œæˆåŠŸ', 'data': {'request_id': '8065132984818443914', 
            # 'task_id': '8065132984818443914', 'task_status': 'SUCCESS', 'choices': [{'role': 'assistant', 
            # 'content': '" æˆ‘æ˜¯ä¸€ä¸ªåä¸ºæ™ºè°±æ¸…è¨€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 
            # å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚"'}], 
            # 'usage': {'prompt_tokens': 3, 'completion_tokens': 53, 'total_tokens': 56}}, 'success': True}
            # or:
            # {'code': 1261, 'msg': 'Prompt è¶…é•¿', 'success': False}
            logger.debug(response)

            if response['code'] == 200:
                return {
                    "total_tokens": response["data"]["usage"]["total_tokens"],
                    "completion_tokens": response["data"]["usage"]["completion_tokens"],
                    "content": str(response["data"]["choices"][0]["content"]).replace('  ', '').replace('"', '').replace('\n', '').replace('\\n', '')
                }
            else:
                return {
                    "completion_tokens": 0, 
                    "content": str('å¯¹ä¸èµ·ï¼Œå‡ºé”™äº†ï¼Œé”™è¯¯ä»£ç ä¸ºï¼š' + str(response['code']) + 'ï¼Œé”™è¯¯ä¿¡æ¯ä¸ºï¼š' + response['msg'])
                }
        except Exception as e:
            need_retry = retry_count < 2
            result = {"completion_tokens": 0, "content": "æˆ‘ç°åœ¨æœ‰ç‚¹ç´¯äº†ï¼Œç­‰ä¼šå†æ¥å§"}
            
            if need_retry:
                logger.warn("[ChatGLM] ç¬¬{}æ¬¡é‡è¯•".format(retry_count + 1))
                return self.reply_text(session, retry_count + 1)
            else:
                return result
        
