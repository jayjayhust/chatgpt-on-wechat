# https://chenzy96.github.io/2018/12/07/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E7%88%AC%E5%8F%96%E5%AE%9E%E6%88%98/
# https://github.com/ChenZY96/SpiderStudy/blob/master/WeChat2.py

### Import 
import requests
from pyquery import PyQuery as pq
import random
import os
# import sys
# import pinecone
# from openai.embeddings_utils import get_embedding
# from tqdm import tqdm
import os
import openai
from common import const
from config import conf
from common.log import logger
import json
import zhipuai


openai.api_key = os.environ.get('OPENAI_API_KEY')
# url = 'https://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&amp;mid=2653006323&amp;idx=1&amp;sn=f94d990fb21c48dbe624ca8af797dd95&amp;chksm=7e54d44549235d537b71936385034f57c43dbd6cc166624e025addd2916cd619cb64ea3aaa17&amp;mpshare=1&amp;scene=24&amp;srcid=0808Er0VbXRrVDMDPe7odDI2&amp;sharer_sharetime=1691469790478&amp;sharer_shareid=5f20824cdd249b7ae9706d1e450be83b#rd'
# url = 'https://mp.weixin.qq.com/s/nyp8BaZqEKYm0jHHsHwIUQ'
# url = sys.argv[1]


class text_abstract(object):
    def __init__(self):
        pass
        
        
    def get_web_text(self, url):
        headers = [
            {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:34.0) Gecko/20100101 Firefox/34.0'},
            {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'},
            {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'},
            {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'},
            {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:40.0) Gecko/20100101 Firefox/40.0'},
            {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/44.0.2403.89 Chrome/44.0.2403.89 Safari/537.36'}
        ]

        response = requests.get(url, headers=random.choice(headers))
        if response.status_code != 200:
            print('ERROR')
            exit(0)
        doc = pq(response.text)

        # æ–‡ç« æ ‡é¢˜
        title = doc.find('.rich_media_title').text()
        # å¾®ä¿¡å…¬ä¼—å·
        author = doc.find('#meta_content .rich_media_meta_text').text()
        source = doc.find('#js_name').text()
        source_info = doc.find('.profile_meta_value').text()

        print('*' * 100)
        print(title, author, source)
        print('*' * 100)
        print(source_info)

        # æ­£æ–‡å†…å®¹
        content = doc.find('.rich_media_content')

        # PMA(ä¸»æˆåˆ†åˆ†æï¼Œç¡®å®š)
        print('*' * 100)
        print('<span> number: ', content.find('span').length)  # æœ‰çš„æ–‡ç« ï¼Œæ–‡æœ¬ä¸»è¦æ”¾åœ¨spanæ ‡ç­¾å†…
        span_pure_text = ''
        for item in content.find('span').items():
            span_pure_text += str(item.text()) + '\n'
        print('*' * 100)
        print('<span> text in total: ', len(span_pure_text)) 
        print('*' * 100)
        print('<p> number: ', content.find('p').length)  # æœ‰çš„æ–‡ç« ï¼Œæ–‡æœ¬ä¸»è¦æ”¾åœ¨pæ ‡ç­¾å†…
        p_pure_text = ''
        for item in content.find('p').items():
            p_pure_text += str(item.text()) + '\n'
        print('*' * 100)
        print('<p> text in total: ', len(p_pure_text)) 
        print('*' * 100)
        print('<section> number: ', content.find('section').length)  # æœ‰çš„æ–‡ç« ï¼Œæ–‡æœ¬ä¸»è¦æ”¾åœ¨sectionæ ‡ç­¾å†…
        section_pure_text = ''
        for item in content.find('section').items():
            if not section_pure_text.__contains__(item.text()):
                section_pure_text += str(item.text())
        section_pure_text = section_pure_text.strip()
        print('*' * 100)
        print('<section> text in total: ', len(section_pure_text))

        # content_pure_text = ''
        # if len(span_pure_text) > len(p_pure_text):
        #     content_pure_text = span_pure_text
        # elif len(p_pure_text) > len(span_pure_text):
        #     content_pure_text = p_pure_text
        content_pure_text = ''
        if len(span_pure_text) > len(p_pure_text) and len(span_pure_text) > len(section_pure_text):
            content_pure_text = span_pure_text
        elif len(p_pure_text) > len(span_pure_text) and len(p_pure_text) > len(section_pure_text):
            content_pure_text = p_pure_text
        elif len(section_pure_text) > len(span_pure_text) and len(section_pure_text) > len(p_pure_text):
            content_pure_text = section_pure_text
        
        return content_pure_text
    

    def get_text_abstract(self, query):
        # prompt = "ã€Šæ¸…åå›¢é˜Ÿé¢†è¡”æ‰“é€ ï¼Œé¦–ä¸ªAI agentç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•é—®ä¸–ã€‹ æ‘˜è¦å¦‚ä¸‹ï¼š\
        #   â–ä¸€å¥è¯æè¿° \
        #   æ¸…åå¤§å­¦ã€ä¿„äº¥ä¿„å·ç«‹å¤§å­¦ã€åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡çš„ç ”ç©¶å›¢é˜Ÿæå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•â€”â€”AgentBenchï¼Œç”¨æ¥è¯„ä¼° LLMs ä½œä¸ºæ™ºèƒ½ä½“åœ¨å„ç§çœŸå®ä¸–ç•ŒæŒ‘æˆ˜å’Œ8ä¸ªä¸åŒç¯å¢ƒä¸­çš„è¡¨ç°ã€‚\
        #   â–æ–‡ç« ç•¥è¯» \
        #   1. AgentBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMsä½œä¸ºä»£ç†åœ¨å„ç§çœŸå®ä¸–ç•ŒæŒ‘æˆ˜å’Œ8ä¸ªä¸åŒç¯å¢ƒä¸­çš„è¡¨ç°çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚ \
        #   2. æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒåƒGPT-4è¿™æ ·çš„é¡¶å°–æ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§å„æ ·çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œè€Œå¤§å¤šæ•°å¼€æºLLMsåœ¨AgentBenchä¸­çš„è¡¨ç°è¿œè¿œä¸åŠåŸºäºAPIçš„LLMsã€‚ \
        #   3. ç ”ç©¶å›¢é˜Ÿå»ºè®®ï¼Œæœ‰å¿…è¦è¿›ä¸€æ­¥åŠªåŠ›æé«˜å¼€æºLLMsçš„å­¦ä¹ èƒ½åŠ›ã€‚ \
        #   4. AIä»£ç†å·²ç»å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›å’Œå¸‚åœºï¼Œç¬¬ä¸€æ‰¹èƒ½å¤Ÿå¯é åœ°æ‰§è¡Œå¤šæ­¥éª¤ä»»åŠ¡å¹¶å…·å¤‡ä¸€å®šè‡ªä¸»èƒ½åŠ›çš„ç³»ç»Ÿå°†åœ¨ä¸€å¹´å†…ä¸Šå¸‚ã€‚ \
        #   5. éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘ä»¬æœ‰æœ›åœ¨ä¸æ–­ä¼˜åŒ–å’Œå®Œå–„ä¸­è§è¯è¿™äº›AIä»£ç†ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥ç§¯æè€Œæ·±è¿œçš„å½±å“ã€‚ \
        #   åŸæ–‡å…±2485å­—ï¼Œé˜…è¯»éœ€4åˆ†é’Ÿ"
        prompt = "ä½ ç°åœ¨è§’è‰²æ˜¯ä¸€ä½é˜…è¯»å°åŠ©æ‰‹ï¼Œè¯·ç»™ä¸‹é¢è¿™æ®µæ–‡å­—ï¼Œç”Ÿæˆä¸€æ®µ200~300å­—å·¦å³çš„æ‘˜è¦ï¼Œå†ç”Ÿæˆä¸€æ®µ100~200å­—å·¦å³çš„æ–‡å­—ã€åˆ—å‡ºæ–‡ç« ä¸­æœ€é‡è¦çš„å‡ ç‚¹ï¼š"
        prompt += "\n"
        prompt += query
        print(prompt)
        print('*' * 100)
        model_type = conf().get("model")
        if model_type in ["gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-4"]:
            res = openai.ChatCompletion.create(
                messages = [{"role": "user", "content": prompt}],
                model=model_type
            )
          
            return res.choices[0].message.content
        if model_type in ["chatglm_pro", "chatglm_std", "chatglm_lite", "chatglm_turbo", "ernie_bot_turbo"]:
            # return "Hi, æˆ‘æ˜¯æ™ºè°±AI(GhatGLM)æ–‡æ‘˜å°åŠ©æ‰‹ï¼Œè¿˜åœ¨å¼€å‘ä¸­å“Ÿï¼Œæ•¬è¯·æœŸå¾…~"
            zhipuai.api_key = conf().get("zhipu_api_key")
            response = zhipuai.model_api.invoke(
                # model="chatglm_lite",  # ChatGLM-6B(https://open.bigmodel.cn/doc/api#chatglm_lite)
                # model="chatglm_std",  # ChatGLM(https://open.bigmodel.cn/doc/api#chatglm_std)
                # model="chatglm_pro",  # ChatGLM(https://open.bigmodel.cn/doc/api#chatglm_pro)
                model="chatglm_turbo",
                prompt=[
                    {"role": "user", "content": "ä½ æ˜¯è°"},  # - user æŒ‡ç”¨æˆ·è§’è‰²è¾“å…¥çš„ä¿¡æ¯
                    {"role": "assistant", "content": conf().get("self_desc")},  # - assistant æŒ‡æ¨¡å‹è¿”å›çš„ä¿¡æ¯
                    {"role": "user", "content": prompt}],
                top_p=0.7,
                temperature=0.9,
            )
            # responseå½¢å¦‚ï¼š
            # {'code': 200, 'msg': 'æ“ä½œæˆåŠŸ', 'data': {'request_id': '8065132984818443914', 
            # 'task_id': '8065132984818443914', 'task_status': 'SUCCESS', 'choices': [{'role': 'assistant', 
            # 'content': '" æˆ‘æ˜¯ä¸€ä¸ªåä¸ºæ™ºè°±æ¸…è¨€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 
            # å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚"'}], 
            # 'usage': {'prompt_tokens': 3, 'completion_tokens': 53, 'total_tokens': 56}}, 'success': True}
            # or:
            # {'code': 1261, 'msg': 'Prompt è¶…é•¿', 'success': False}
            logger.debug(response)

            if response['code'] == 200:
                return str(response["data"]["choices"][0]["content"]).replace('  ', '').replace('"', '').replace('\n', '').replace('\\n\\n', '\n').replace('\\n', '\n')
        # if model_type in ["ernie_bot", "ernie_bot_turbo"]:
        #     # return "Hi, æˆ‘æ˜¯æ–‡å¿ƒä¸€è¨€(ERNIE)æ–‡æ‘˜å°åŠ©æ‰‹ï¼Œè¿˜åœ¨å¼€å‘ä¸­å“Ÿï¼Œæ•¬è¯·æœŸå¾…~"
        #     access_key = conf().get("baidu_ernie_access_key")
        #     secret_key = conf().get("baidu_ernie_secret_key")
        #     url  = "https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=" + access_key + "&client_secret=" + secret_key
        #     payload = json.dumps("")
        #     headers = {
        #         'Content-Type': 'application/json',
        #         'Accept': 'application/json'
        #     }
            
        #     response = requests.request("POST", url, headers=headers, data=payload)
        #     access_token = response.json().get("access_token")

        #     url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token=" + access_token
        #     payload = json.dumps({
        #         #  æœ€åä¸€ä¸ªmessageçš„contenté•¿åº¦ï¼ˆå³æ­¤è½®å¯¹è¯çš„é—®é¢˜ï¼‰ä¸èƒ½è¶…è¿‡11200ä¸ªå­—ç¬¦ï¼›
        #         #  å¦‚æœmessagesä¸­contentæ€»é•¿åº¦å¤§äº11200å­—ç¬¦ï¼Œç³»ç»Ÿä¼šä¾æ¬¡é—å¿˜æœ€æ—©çš„å†å²ä¼šè¯ï¼Œç›´åˆ°contentçš„æ€»é•¿åº¦ä¸è¶…è¿‡11200ä¸ªå­—ç¬¦
        #         "messages": [
        #             {
        #                 "role": "user",
        #                 "content": prompt
        #             }
        #         ]
        #     })
        #     headers = {
        #         'Content-Type': 'application/json'
        #     }
            
        #     response = requests.request("POST", url, headers=headers, data=payload)
        #     if response:
        #         # return "ä»¥ä¸‹å›å¤æ¥è‡ªæ–‡å¿ƒä¸€è¨€(ERNIE)ï¼š" + response.json()["result"]
        #         return response.json()["result"]

